{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Bll7JXXVwu-"
      },
      "outputs": [],
      "source": [
        "!pip install git+https://github.com/gautamchitnis/cocoapi.git@cocodataset-master#subdirectory=PythonAPI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T4JEK5M1t9Ee"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pUoNOWtGUQKe"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.chdir(\"/content/drive/MyDrive/SB\")\n",
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hnkERF6lvLfS"
      },
      "outputs": [],
      "source": [
        "device = \"cuda\"\n",
        "EPOCHS = 6\n",
        "BATCH_SIZE = 8\n",
        "NUM_WORKERS = 2\n",
        "IMAGE_HEIGHT = 256\n",
        "IMAGE_WIDTH = 256\n",
        "TRAIN_IMAGE_PATH = '/content/drive/MyDrive/SB/ears/train/'\n",
        "TRAIN_MASK_PATH = '/content/drive/MyDrive/SB/ears/annotations/segmentation/train/'\n",
        "TRAIN_YOLO_PATH = '/content/drive/MyDrive/SB/ears/annotations/detection/train_YOLO_format/'\n",
        "\n",
        "TEST_IMAGE_PATH = '/content/drive/MyDrive/SB/ears/test/'\n",
        "TEST_MASK_PATH = '/content/drive/MyDrive/SB/ears/annotations/segmentation/test/'\n",
        "TEST_YOLO_PATH = '/content/drive/MyDrive/SB/ears/annotations/detection/test_YOLO_format/'\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hX4FT3wjkTIO"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "from matplotlib import pyplot as plt\n",
        "%matplotlib notebook\n",
        "%matplotlib inline\n",
        "\n",
        "class EarDataset(Dataset):\n",
        "    def __init__(self, image_dir, mask_dir, yolo_dir, transform):\n",
        "        self.image_dir = image_dir\n",
        "        self.mask_dir = mask_dir\n",
        "        self.yolo_dir = yolo_dir\n",
        "        self.transform = transform\n",
        "        self.imgs = list(sorted(os.listdir(image_dir)))\n",
        "        self.masks = list(sorted(os.listdir(mask_dir)))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        img_path = os.path.join(self.image_dir, self.imgs[item])\n",
        "        mask_path = os.path.join(self.mask_dir, self.masks[item])\n",
        "        yolo_path = os.path.join(self.yolo_dir, self.imgs[item].replace(\"png\", \"txt\"))\n",
        "        yolo_boxes = []\n",
        "        with open(yolo_path) as file:\n",
        "            yolo_boxes.append(file.readline().split(\" \")[1:])\n",
        "        box = yolo_boxes[0]       \n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "        #image = np.array(image)\n",
        "        #img = self.preprocess.pre(image)\n",
        "        #image = Image.fromarray(img)\n",
        "        mask = Image.open(mask_path)\n",
        "        \n",
        "        mask = np.array(mask)[None, :, :] == 255\n",
        "        \n",
        "        \n",
        "\n",
        "\n",
        "        # get bounding box coordinates for each mask\n",
        "        \n",
        "        \n",
        "        boxes = []\n",
        "        for box in yolo_boxes:\n",
        "            xmin = int(box[0])\n",
        "            xmax = int(box[0]) + int(box[2])\n",
        "            ymin = int(box[1])\n",
        "            ymax = int(box[1]) + int(box[3])\n",
        "            boxes.append([xmin, ymin, xmax, ymax])\n",
        "\n",
        "        num_objs = len(boxes)\n",
        "        # convert everything into a torch.Tensor\n",
        "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
        "        # there is only one class\n",
        "        \n",
        "        labels = torch.ones((num_objs,), dtype=torch.int64)\n",
        "        masks = torch.as_tensor(mask, dtype=torch.uint8)\n",
        "        image_id = torch.tensor([item])\n",
        "        \n",
        "        \n",
        "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
        "        # suppose all instances are not crowd\n",
        "        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n",
        "\n",
        "        target = {}\n",
        "        target[\"boxes\"] = boxes\n",
        "        target[\"labels\"] = labels\n",
        "        target[\"masks\"] = masks\n",
        "        target[\"image_id\"] = image_id\n",
        "        target[\"area\"] = area\n",
        "        target[\"iscrowd\"] = iscrowd\n",
        "\n",
        "        if self.transform is not None:\n",
        "            image, target= self.transform(image, target)\n",
        "            \n",
        "\n",
        "           \n",
        "        return image, target\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.imgs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JyW87s-6qZUm"
      },
      "outputs": [],
      "source": [
        "import torchvision\n",
        "from torchvision.models.detection.rpn import AnchorGenerator\n",
        "from torchvision.models.detection import MaskRCNN\n",
        "\n",
        "\n",
        "def get_model_instance_segmentation(num_classes):\n",
        "    # load an instance segmentation model pre-trained on COCO\n",
        "    bbone = torchvision.models.mobilenet_v2(pretrained=True).features\n",
        "            \n",
        "    bbone.out_channels = 1280\n",
        "\n",
        "    \n",
        "    anchor_generator = AnchorGenerator(sizes=((32, 64, 128, 256, 512),),\n",
        "                                        aspect_ratios=((0.5, 1.0, 2.0),))\n",
        "\n",
        "    \n",
        "    roi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=['0'],\n",
        "                                                    output_size=7,\n",
        "                                                    sampling_ratio=2)\n",
        "\n",
        "    mask_roi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=['0'],\n",
        "                                                          output_size=14,\n",
        "                                                          sampling_ratio=2)\n",
        "\n",
        "    model = MaskRCNN(bbone,\n",
        "                          num_classes=2,\n",
        "                          rpn_anchor_generator=anchor_generator,\n",
        "                          box_roi_pool=roi_pooler,\n",
        "                          mask_roi_pool=mask_roi_pooler)\n",
        "    \n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gahqILM7vsxA",
        "outputId": "2c9039d3-61f5-449c-edc4-5a0592ddd57c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "fatal: destination path 'vision' already exists and is not an empty directory.\n",
            "shell-init: error retrieving current directory: getcwd: cannot access parent directories: No such file or directory\n",
            "shell-init: error retrieving current directory: getcwd: cannot access parent directories: No such file or directory\n",
            "shell-init: error retrieving current directory: getcwd: cannot access parent directories: No such file or directory\n"
          ]
        }
      ],
      "source": [
        "# Download TorchVision repo to use some files from\n",
        "# references/detection\n",
        "!git clone https://github.com/pytorch/vision.git\n",
        "#!cd vision\n",
        "\n",
        "!cp vision/references/detection/utils.py ./\n",
        "!cp vision/references/detection/coco_eval.py ./\n",
        "!cp vision/references/detection/engine.py ./\n",
        "!cp vision/references/detection/coco_utils.py ./"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "83u3qfseiQqB",
        "outputId": "73c8d080-c590-40a2-da93-33bbe5b020b2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "shell-init: error retrieving current directory: getcwd: cannot access parent directories: No such file or directory\n",
            "cp: '/content/drive/MyDrive/SB/preprocess.py' and './preprocess.py' are the same file\n",
            "cp: '/content/drive/MyDrive/SB/transforms.py' and './transforms.py' are the same file\n"
          ]
        }
      ],
      "source": [
        "\n",
        "!cp /content/drive/MyDrive/SB/preprocess.py ./\n",
        "!cp /content/drive/MyDrive/SB/transforms.py ./\n",
        "import transforms as T\n",
        "\n",
        "\n",
        "def get_transform(train):\n",
        "    transforms = []\n",
        "    # converts the image, a PIL image, into a PyTorch Tensor\n",
        "    transforms.append(T.HistogramEqualization())\n",
        "    #transforms.append(T.ImageSharpening())\n",
        "    transforms.append(T.ToTensor())\n",
        "    return T.Compose(transforms)\n",
        "\n",
        "def store_predictions(model, test_data_loader):\n",
        "    data_iter = iter(test_data_loader)\n",
        "\n",
        "    model.eval()\n",
        "    i = 0\n",
        "    for images, targets in data_iter:\n",
        "        images = list(image for image in images)\n",
        "        outputs = model(images)\n",
        "        outputs = outputs[0][\"masks\"]\n",
        "        masks = [mask.to(torch.device('cpu')).detach().numpy()[0] for mask in outputs]\n",
        "        mask = np.zeros(masks[0].shape)\n",
        "        for m in masks:\n",
        "            mask = np.logical_or(mask, m > 0.5)\n",
        "        mask = mask.astype(\"uint8\")\n",
        "        mask *= 255\n",
        "        im = Image.fromarray(mask, mode='L').convert('1')\n",
        "        im.save(\"/content/drive/MyDrive/SB/ears/predictions\"f\"/{i+1:04d}\"+\".png\")\n",
        "        i += 1\n",
        "                  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NoyzWmq_qe4T"
      },
      "outputs": [],
      "source": [
        "from engine import train_one_epoch, evaluate\n",
        "import utils\n",
        "import torch\n",
        "\n",
        "\n",
        "def main():\n",
        "    # train on the GPU or on the CPU, if a GPU is not available\n",
        "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "    # our dataset has two classes only - background and person\n",
        "    num_classes = 2\n",
        "    # use our dataset and defined transformations\n",
        "    dataset = EarDataset(TRAIN_IMAGE_PATH, TRAIN_MASK_PATH, TRAIN_YOLO_PATH, get_transform(train=True))\n",
        "    dataset_test = EarDataset(TEST_IMAGE_PATH, TEST_MASK_PATH, TEST_YOLO_PATH, get_transform(train=False))\n",
        "\n",
        "    # define training and validation data loaders\n",
        "    data_loader = torch.utils.data.DataLoader(\n",
        "        dataset, batch_size=2, shuffle=False, num_workers=4,\n",
        "        collate_fn=utils.collate_fn)\n",
        "\n",
        "    data_loader_test = torch.utils.data.DataLoader(\n",
        "        dataset_test, batch_size=1, shuffle=False, num_workers=4,\n",
        "        collate_fn=utils.collate_fn)\n",
        "\n",
        "    # get the model using our helper function\n",
        "    model = get_model_instance_segmentation(num_classes)\n",
        "\n",
        "    # move model to the right device\n",
        "    model.to(device)\n",
        "\n",
        "    # construct an optimizer\n",
        "    params = [p for p in model.parameters() if p.requires_grad]\n",
        "    optimizer = torch.optim.Adam(params, lr=1e-5, weight_decay=1e-2)\n",
        "    # and a learning rate scheduler\n",
        "    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
        "                                                   step_size=3,\n",
        "                                                   gamma=0.1)\n",
        "\n",
        "    # let's train it for 10 epochs\n",
        "    num_epochs = 10\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # train for one epoch, printing every 10 iterations\n",
        "        train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10)\n",
        "        # update the learning rate\n",
        "        lr_scheduler.step()\n",
        "        #store_predictions(model, data_loader_test)\n",
        "\n",
        "        #save the model\n",
        "        torch.save(model.state_dict(), \"model3_weights_epoch{}.pth\".format(epoch))\n",
        "\n",
        "    print(\"That's it!\")\n",
        "main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bUOpZs3jj7M9"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "def iou_compute(p, gt):\n",
        "    # Computes Intersection Over Union (IOU)\n",
        "    if len(p) == 0:\n",
        "        return 0\n",
        "\n",
        "    intersection = np.logical_and(p, gt)\n",
        "    union = np.logical_or(p, gt)\n",
        "\n",
        "    iou = np.sum(intersection) / np.sum(union)\n",
        "\n",
        "    return iou \n",
        "\n",
        "def dice_score(p, gt):\n",
        "    num_correct = (p == gt).sum()\n",
        "    num_pixels = torch.numel()\n",
        "     \n",
        "\n",
        "def F1(p, gt):\n",
        "    true_positive = (p == 255.0 and gt == 255.0).sum()\n",
        "    false_positive = (p == 255.0 and gt == 0.0).sum()\n",
        "    false_negative = (p == 0.0 and gt == 255.0).sum()\n",
        "    presision = true_positive / (true_positive + false_positive)\n",
        "    recall true_positive / (true_positive + false_negative)\n",
        "\n",
        "    return 2 * recall * precision / (recall + precision)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-8C8PSMllArQ"
      },
      "outputs": [],
      "source": [
        "from matplotlib import pyplot as plt\n",
        "import torch\n",
        "dataset_test = EarDataset(TEST_IMAGE_PATH, TEST_MASK_PATH, TEST_YOLO_PATH, get_transform(train=False), preprocess)\n",
        "data_loader_test = torch.utils.data.DataLoader(\n",
        "        dataset_test, batch_size=1, shuffle=False, num_workers=2,\n",
        "        collate_fn=utils.collate_fn)\n",
        "\n",
        "# get the model using our helper function\n",
        "num_classes=2\n",
        "model = get_model_instance_segmentation(num_classes)\n",
        "model.load_state_dict(torch.load(\"model3_weights_epoch4.pth\"))\n",
        "# move model to the right device\n",
        "model.to(device)\n",
        "store_predictions(model, data_loader_test)\n",
        "     \n",
        "length = len(dataset_test)\n",
        "# put the model in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "iou_all = 0\n",
        "for i, item in enumerate(dataset_test):\n",
        "    with torch.no_grad():\n",
        "        prediction = model([item[0].to(device)])\n",
        "    iou = iou_compute(prediction[0][\"masks\"].cpu().numpy(), item[1]['masks'].cpu().numpy())\n",
        "    plt.imsave   \n",
        "    print(iou) \n",
        "    iou_all += iou\n",
        "\n",
        "print(iou_all/length)\n",
        "\n",
        "#Image.fromarray(prediction[0]['masks'][0, 0].mul(255).byte().cpu().numpy())\n",
        "#for img, _ in enumerate(dataset_test):\n",
        "#  model.eval()\n",
        "#  with torch.no_grad():\n",
        "##    prediction = model([img.to(device)])\n",
        "#   Image.fromarray(prediction[0]['masks'][0, 0].mul(255).byte().cpu().numpy())\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NEDvrV4xOW4W"
      },
      "outputs": [],
      "source": [
        "def run_evaluation():\n",
        "\n",
        "    iou_arr = []\n",
        "    model = get_model_instance_segmentation(num_classes=2)\n",
        "    model.load_state_dict(torch.load(\"model3_weights_epoch3.pth\"))\n",
        "    \n",
        "    \n",
        "\n",
        "    for i in range(250):\n",
        "        # Read an image\n",
        "        img = cv2.imread(\"/content/drive/MyDrive/SB/ears/predictions\"+f\"/{i+1:04d}\"+\".png\")\n",
        "        gt = cv2.imread(\"/content/drive/MyDrive/SB/ears/annotations/segmentation/test\"+f\"/{i+1:04d}\"+\".png\")\n",
        "        \n",
        "        iou = iou_compute(img, gt)\n",
        "        iou_arr.append(iou)\n",
        "\n",
        "    miou = np.average(iou_arr)\n",
        "    print(\"\\n\")\n",
        "    print(\"Average IOU:\", f\"{miou:.2%}\")\n",
        "    print(\"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zMzjZiVKMqDW"
      },
      "outputs": [],
      "source": [
        "run_evaluation()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Untitled0.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
