{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "UhvwWkaRbhCe"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!python --version\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "njP0FvCrdJrF"
      },
      "source": [
        " #!cp -r '/content/drive/MyDrive/SB' '/content/SB'\n",
        " !nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FL5evcn5jnmY"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.utils.data as data\n",
        "import torchvision\n",
        "from torchvision import transforms\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PGApxatybJ1m"
      },
      "source": [
        "device = \"cuda\"\n",
        "EPOCHS = 2\n",
        "BATCH_SIZE = 8\n",
        "NUM_WORKERS = 2\n",
        "IMAGE_HEIGHT = 256\n",
        "IMAGE_WIDTH = 256\n",
        "TRAIN_IMAGE_PATH = '/content/drive/MyDrive/SB/ears/train/'\n",
        "TRAIN_MASK_PATH = '/content/drive/MyDrive/SB/ears/annotations/segmentation/train/'\n",
        "\n",
        "TEST_IMAGE_PATH = '/content/drive/MyDrive/SB/ears/test/'\n",
        "TEST_MASK_PATH = '/content/drive/MyDrive/SB/ears/annotations/segmentation/test/'\n",
        "transform_img = transforms.Compose(\n",
        "    [transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "     transforms.ToTensor()])\n",
        "\n",
        "transform_eval = transforms.Compose(\n",
        "    [transforms.Normalize((0.5), (0.5)),\n",
        "     transforms.ToTensor()])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J8rMWXgLgpR_"
      },
      "source": [
        "class DoubleConv(nn.Module):\n",
        "  def __init__(self, in_channels, out_channels):\n",
        "    super(DoubleConv, self).__init__()\n",
        "    self.conv = nn.Sequential(\n",
        "        nn.Conv2d(in_channels, out_channels, 3, 1, 1, bias=False),\n",
        "        nn.BatchNorm2d(out_channels),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.Conv2d(out_channels, out_channels, 3, 1, 1, bias=False),\n",
        "        nn.BatchNorm2d(out_channels),\n",
        "        nn.ReLU(inplace=True),\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.conv(x)\n",
        "\n",
        "\n",
        "class UNET(nn.Module):\n",
        "  def __init__(self, in_channels=3, out_channels=1\n",
        "               , features = [64, 128, 256, 512]):\n",
        "    super(UNET, self).__init__()\n",
        "\n",
        "    self.ups = nn.ModuleList()\n",
        "    self.downs = nn.ModuleList()\n",
        "    self.pool = nn.MaxPool2d(kernel_size=3, stride=3)\n",
        "\n",
        "    #Down part of UNET\n",
        "    for feature in features:\n",
        "      self.downs.append(DoubleConv(in_channels, feature))\n",
        "      in_channels = feature\n",
        "\n",
        "    #Up part of UNET\n",
        "    for feature in reversed(features):\n",
        "        self.ups.append(\n",
        "            nn.ConvTranspose2d(\n",
        "                feature*2,\n",
        "                feature,\n",
        "                kernel_size=3,\n",
        "                stride=3\n",
        "            )\n",
        "        )\n",
        "        self.ups.append(DoubleConv(feature*2, feature))\n",
        "\n",
        "    #bottom layer\n",
        "    self.bottleneck = DoubleConv(features[-1], features[-1]*2)   \n",
        "    self.final_conv = nn.Conv2d(features[0], out_channels, kernel_size=1)\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    skip_connections = []\n",
        "\n",
        "    for down in self.downs:\n",
        "      x = down(x)\n",
        "      skip_connections.append(x)\n",
        "      x = self.pool(x)\n",
        "\n",
        "    x = self.bottleneck(x)\n",
        "    #reverse\n",
        "    skip_connections = skip_connections[::-1]\n",
        "\n",
        "    #2 steps - up (convTranspose) and double conv\n",
        "    for idx in range(0, len(self.ups), 2):\n",
        "      x = self.ups[idx](x)\n",
        "      # divided by 2 cause we do 2 steps at a time\n",
        "      skip_connection = skip_connections[idx//2]\n",
        "\n",
        "\n",
        "      #if shapes dont match\n",
        "      if x.shape != skip_connection.shape:\n",
        "        #skiping batch size and num of channels for skip_connection\n",
        "        x = transforms.functional.resize(x, size=skip_connection.shape[2:])\n",
        "\n",
        "\n",
        "      concat_skip = torch.cat((skip_connection, x), dim=1)\n",
        "      x = self.ups[idx+1](concat_skip)\n",
        "\n",
        "    return self.final_conv(x)\n",
        "\n",
        "\n",
        "x = torch.randn((3, 1, 480, 360))\n",
        "model = UNET(in_channels=1, out_channels=1)\n",
        "preds = model(x)\n",
        "print(preds.shape)\n",
        "print(x.shape)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gy_G8sr5czq7"
      },
      "source": [
        "import os\n",
        "import cv2\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset\n",
        "import numpy as np\n",
        "\n",
        "class EarDataset(Dataset):\n",
        "  def __init__(self, image_dir, mask_dir, transform_img=None, transform_eval=None):\n",
        "    self.image_dir = image_dir\n",
        "    self.mask_dir = mask_dir\n",
        "    self.transform_img = transform_img\n",
        "    self.transform_eval = transform_eval\n",
        "    self.images = os.listdir(image_dir)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.images)\n",
        "\n",
        "  def __getitem__(self, item):\n",
        "    img_path = os.path.join(self.image_dir, self.images[item])\n",
        "    mask_path = os.path.join(self.mask_dir, self.images[item])\n",
        "    image = cv2.imread(img_path)\n",
        "    mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
        "    mask[mask == 255.0] = 1.0\n",
        "    image = self.transform_img(image)\n",
        "    mask = self.transform_eval(mask)\n",
        "    print(type(image))\n",
        "    print(type(mask))\n",
        "    return image, mask"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AgD_0PkLif4-"
      },
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "def train_model(loader, model, optimizer, loss_fn, scaler, device):\n",
        "  loop = tqdm(loader)\n",
        "\n",
        "  for batch_idx, (data, targets) in enumerate(loop):\n",
        "    data = data.to(device)\n",
        "    targets = targets.float().to(device)\n",
        "    print(data.shape)\n",
        "    print(targets.shape)\n",
        "\n",
        "    # zero the parameter gradients\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "    outputs = model(data)\n",
        "    loss = criterion(data, targets)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    #update tqdm loop\n",
        "    loop.set_postfix(loss=loss.item())  \n",
        "\n",
        "def check_accuracy(test_loader, model, device):\n",
        "  correct = 0\n",
        "  num_pixels = 0  \n",
        "  model.eval()\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for x, y in loader:\n",
        "      x = x.to(device) \n",
        "      y = y.to(device)\n",
        "      preds = torch.sigmoid(model(x))\n",
        "      preds =(preds > 0.5).float()\n",
        "      num_correct += (preds == y).sum()\n",
        "      num.num_pixels += torch.numel(preds)\n",
        "\n",
        "  print( f\"Got {correct}/{num_pixels} with acc {correct/num_pixels:.2f}\")    \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mEyElqnGwBWI"
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "def get_loaders(train_img_path,\n",
        "                train_mask_path,\n",
        "                test_img_path,\n",
        "                test_mask_path,\n",
        "                batch_size,\n",
        "                transform_img,\n",
        "                transform_eval,\n",
        "                num_workers):\n",
        "  train_ds = EarDataset(\n",
        "      image_dir=train_img_path,\n",
        "      mask_dir=train_mask_path,\n",
        "      transform_img=transform_img,\n",
        "      transform_eval=transform_eval\n",
        "  )\n",
        "\n",
        "  train_loader = DataLoader(\n",
        "      train_ds,\n",
        "      batch_size=batch_size,\n",
        "      num_workers=num_workers,\n",
        "  )\n",
        "\n",
        "  test_ds = EarDataset(\n",
        "      image_dir=test_img_path,\n",
        "      mask_dir=test_mask_path,\n",
        "      transform_img=transform_img,\n",
        "      transform_eval=transform_eval\n",
        "  )\n",
        "\n",
        "  test_loader = DataLoader(\n",
        "      test_ds,\n",
        "      batch_size=batch_size,\n",
        "      num_workers=num_workers,\n",
        "  )\n",
        "  return train_loader, test_loader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xGGOrhS0zpHY"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from torch import optim\n",
        "model = UNET(in_channels=3, out_channels=1).to(device)    \n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "train_loader, test_loader = get_loaders(\n",
        "    TRAIN_IMAGE_PATH,\n",
        "    TRAIN_MASK_PATH,\n",
        "    TEST_IMAGE_PATH,\n",
        "    TEST_MASK_PATH,\n",
        "    BATCH_SIZE,\n",
        "    transform_img,\n",
        "    transform_eval,\n",
        "    NUM_WORKERS\n",
        ")\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  train_model(train_loader, model, optimizer, loss_fn, scaler, device)\n",
        "\n",
        "  #save model\n",
        "  checkpoint = {\n",
        "      \"state_dict\": model.state_dict(),\n",
        "      \"optimizer\": optimizer.state_dict(),\n",
        "  }\n",
        "  #save_checkpoint(checkpoint)\n",
        "\n",
        "  #check accuracy\n",
        "  check_accuracy(test_loader, model, device)\n",
        "\n",
        "  #save_prediction_as_imgs(train_loader, model)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}